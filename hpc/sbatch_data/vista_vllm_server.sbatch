#!/bin/bash
#SBATCH -p {partition}
#SBATCH --time={time_limit}
#SBATCH --nodes {num_nodes}
#SBATCH --ntasks-per-node 1
#SBATCH --cpus-per-task={cpus_per_node}
#SBATCH --exclude=c610-021,c611-011,c640-041,c611-041,c611-122,c637-082
#SBATCH --account {account}
#SBATCH --output={experiments_dir}/logs/%x_%j.out
#SBATCH --job-name={job_name}
#SBATCH --mail-type=END,TIME_LIMIT,FAIL
#SBATCH --mail-user=bf996@nyu.edu

set -eo pipefail

module load gcc/15.1.0
module load cuda/12.8
module load tacc-apptainer

# =============================================================================
# Create conda activation script for Ray worker environment consistency
# This ensures spawned processes (Ray actors, multiprocessing workers) get
# the same environment as the main process.
# =============================================================================
echo "=== Setting up Conda Activation Script for Ray Workers ==="
CONDA_ENV_PATH="$SCRATCH/miniconda3/envs/vllm_sandboxes"
ACTIVATE_D="$CONDA_ENV_PATH/etc/conda/activate.d"
ACTIVATION_SCRIPT="$ACTIVATE_D/vllm_ray_env.sh"

mkdir -p "$ACTIVATE_D"

# Detect libcuda.so path for Triton
TRITON_LIBCUDA_PATH=""
for candidate in \
    "/usr/lib64/libcuda.so.1" \
    "/usr/lib64/libcuda.so" \
    "/usr/local/cuda/lib64/stubs/libcuda.so" \
    "/opt/nvidia/lib64/libcuda.so.1"; do
    if [ -f "$candidate" ]; then
        TRITON_LIBCUDA_PATH="$candidate"
        break
    fi
done

# Write the activation script
cat > "$ACTIVATION_SCRIPT" << 'ACTIVATE_EOF'
#!/bin/bash
# =============================================================================
# vLLM Ray Worker Environment Setup
# Auto-generated by vista_vllm_server.sbatch
# This script runs on conda activate to ensure Ray actors get consistent env
# =============================================================================

# Triton CUDA driver detection
if [ -z "${TRITON_LIBCUDA_PATH:-}" ]; then
    for _candidate in \
        "/usr/lib64/libcuda.so.1" \
        "/usr/lib64/libcuda.so" \
        "/usr/local/cuda/lib64/stubs/libcuda.so" \
        "/opt/nvidia/lib64/libcuda.so.1"; do
        if [ -f "$_candidate" ]; then
            export TRITON_LIBCUDA_PATH="$_candidate"
            break
        fi
    done
fi

# Ensure CUDA driver directory is in LD_LIBRARY_PATH
if [ -n "${TRITON_LIBCUDA_PATH:-}" ]; then
    _cuda_driver_dir=$(dirname "$TRITON_LIBCUDA_PATH")
    if [[ ":${LD_LIBRARY_PATH:-}:" != *":$_cuda_driver_dir:"* ]]; then
        export LD_LIBRARY_PATH="$_cuda_driver_dir:${LD_LIBRARY_PATH:-}"
    fi
fi

# GCC/Triton compiler settings (needed for JIT compilation)
if [ -z "${TRITON_CC:-}" ]; then
    if [ -x "/home1/apps/gcc/15.1.0/bin/gcc" ]; then
        export TRITON_CC="/home1/apps/gcc/15.1.0/bin/gcc"
    elif command -v gcc &>/dev/null; then
        export TRITON_CC=$(command -v gcc)
    fi
fi

# Ensure GCC libraries are available
_gcc_lib="/home1/apps/gcc/15.1.0/lib64"
if [ -d "$_gcc_lib" ] && [[ ":${LD_LIBRARY_PATH:-}:" != *":$_gcc_lib:"* ]]; then
    export LD_LIBRARY_PATH="$_gcc_lib:${LD_LIBRARY_PATH:-}"
fi

# Ensure GCC is in PATH
_gcc_bin="/home1/apps/gcc/15.1.0/bin"
if [ -d "$_gcc_bin" ] && [[ ":${PATH:-}:" != *":$_gcc_bin:"* ]]; then
    export PATH="$_gcc_bin:${PATH:-}"
fi

# LD_PRELOAD for C++ stdlib compatibility
_libstdcpp="/home1/apps/gcc/15.1.0/lib64/libstdc++.so.6"
if [ -f "$_libstdcpp" ] && [[ ":${LD_PRELOAD:-}:" != *":$_libstdcpp:"* ]]; then
    export LD_PRELOAD="$_libstdcpp${LD_PRELOAD:+:$LD_PRELOAD}"
fi
ACTIVATE_EOF

chmod +x "$ACTIVATION_SCRIPT"
echo "  Created activation script: $ACTIVATION_SCRIPT"
echo "  TRITON_LIBCUDA_PATH will be: ${TRITON_LIBCUDA_PATH:-<not found>}"

# Now activate conda (which will source our new script)
source $SCRATCH/miniconda3/etc/profile.d/conda.sh
conda activate $SCRATCH/miniconda3/envs/vllm_sandboxes

# Verify the activation script worked
echo "=== Post-Activation Environment Check ==="
echo "  TRITON_CC=${TRITON_CC:-<not set>}"
echo "  TRITON_LIBCUDA_PATH=${TRITON_LIBCUDA_PATH:-<not set>}"
echo "  LD_PRELOAD=${LD_PRELOAD:-<not set>}"
echo "  GCC in PATH: $(which gcc 2>/dev/null || echo '<not found>')"
echo "  libcuda.so exists: $([ -f "${TRITON_LIBCUDA_PATH:-/nonexistent}" ] && echo 'yes' || echo 'no')"

# Log GPU visibility for debugging
echo "  CUDA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES:-<not set>}"
echo "  nvidia-smi GPU count: $(nvidia-smi -L 2>/dev/null | wc -l || echo 'nvidia-smi not available')"
echo "==========================================="

echo $DCFT
source "$DCFT/hpc/dotenv/tacc.env"
source "$SCRATCH/keys.env"

export HF_HOME="/tmp/hf_home"
export PYTHONFAULTHANDLER=1
export NCCL_TIMEOUT=1800  # 30 minutes
export NCCL_IB_TIMEOUT=23  # InfiniBand timeout
export PYTORCH_CUDA_ALLOC_CONF="garbage_collection_threshold:0.6,max_split_size_mb:128"

cd "$DCFT"
export PYTHONPATH="$PWD:${PYTHONPATH:-}"

# Build environment export strings - include TRITON_LIBCUDA_PATH for spawned processes
TRITON_LIBCUDA_EXPORT=""
if [ -n "${TRITON_LIBCUDA_PATH:-}" ]; then
    TRITON_LIBCUDA_EXPORT=",TRITON_LIBCUDA_PATH=$TRITON_LIBCUDA_PATH"
fi
export SRUN_EXPORT_ENV="ALL,TRITON_CC=$TRITON_CC,LD_LIBRARY_PATH=$LD_LIBRARY_PATH,PATH=$PATH,LD_PRELOAD=$LD_PRELOAD,HF_HOME=$HF_HOME,PYTHONPATH=$PYTHONPATH${TRITON_LIBCUDA_EXPORT}"

# For Ray workers (space-separated for env command)
RAY_ENV_VARS="TRITON_CC=$TRITON_CC LD_LIBRARY_PATH=$LD_LIBRARY_PATH PATH=$PATH HF_HOME=$HF_HOME PYTHONPATH=$PYTHONPATH"
if [ -n "${TRITON_LIBCUDA_PATH:-}" ]; then
    RAY_ENV_VARS="$RAY_ENV_VARS TRITON_LIBCUDA_PATH=$TRITON_LIBCUDA_PATH"
fi

BACKEND="${VLLM_BACKEND:-vllm}"

MODEL_PATH="${VLLM_MODEL_PATH:?VLLM_MODEL_PATH is required}"
TP_SIZE="${VLLM_TENSOR_PARALLEL_SIZE:-1}"
PP_SIZE="${VLLM_PIPELINE_PARALLEL_SIZE:-1}"
DP_SIZE="${VLLM_DATA_PARALLEL_SIZE:-1}"
CUSTOM_MODEL_NAME="${VLLM_CUSTOM_MODEL_NAME:-}"
ENDPOINT_JSON="${VLLM_ENDPOINT_JSON_PATH:-$DCFT/vllm_endpoint.json}"
RAY_PORT="${VLLM_RAY_PORT:-6379}"
API_PORT="${VLLM_API_PORT:-8000}"
GPUS_PER_NODE="${VLLM_GPUS_PER_NODE:-${gpus_per_node}}"
export VLLM_GPUS_PER_NODE="$GPUS_PER_NODE"

if [ "$BACKEND" = "ray" ]; then

echo "=== Ray-backed vLLM Server Configuration ==="
echo "Model: $MODEL_PATH"
echo "Tensor Parallel Size: $TP_SIZE"
echo "Custom Model Name: ${CUSTOM_MODEL_NAME:-<none>}"
echo "Endpoint JSON: $ENDPOINT_JSON"
echo "Ray Port: $RAY_PORT"
echo "API Port: $API_PORT"
echo "GPUs per node: $GPUS_PER_NODE"
echo "==========================================="

nodes=$(scontrol show hostnames "$SLURM_JOB_NODELIST")
nodes_array=($nodes)
head_node=${nodes_array[0]}

head_node_ip=$(srun --export="$SRUN_EXPORT_ENV" --nodes=1 --ntasks=1 --overlap -w "$head_node" hostname --ip-address)
head_node_ip=${head_node_ip%% *}

ip_head="${head_node_ip}:${RAY_PORT}"
export RAY_ADDRESS="$ip_head"

ray_pids=()

start_ray_node() {
    local node_name=$1
    local start_cmd=$2
    srun --export="$SRUN_EXPORT_ENV" --nodes=1 --ntasks=1 --overlap -w "$node_name" bash -c "env $RAY_ENV_VARS printenv TRITON_CC TRITON_LIBCUDA_PATH LD_LIBRARY_PATH PATH; env $RAY_ENV_VARS $start_cmd" &
    ray_pids+=($!)
}

head_cmd="ray start --head --node-ip-address=${head_node_ip} --port=${RAY_PORT} --num-gpus=${GPUS_PER_NODE} --num-cpus={cpus_per_node} --block"
echo "Starting Ray head on ${head_node} (${head_node_ip})"
start_ray_node "$head_node" "$head_cmd"

for ((i = 1; i < SLURM_JOB_NUM_NODES; i++)); do
    node_i=${nodes_array[$i]}
    worker_cmd="ray start --address ${ip_head} --num-gpus=${GPUS_PER_NODE} --num-cpus={cpus_per_node} --block"
    echo "Starting Ray worker on ${node_i}"
    start_ray_node "$node_i" "$worker_cmd"
    sleep 3
done

cleanup() {
    echo "Stopping vLLM server and Ray cluster..."
    if [ -n "${VLLM_PID:-}" ]; then
        kill "$VLLM_PID" >/dev/null 2>&1 || true
        wait "$VLLM_PID" >/dev/null 2>&1 || true
    fi
    for node in "${nodes_array[@]}"; do
        srun --export="$SRUN_EXPORT_ENV" --nodes=1 --ntasks=1 --overlap -w "$node" ray stop --force >/dev/null 2>&1 || true
    done
    for pid in "${ray_pids[@]}"; do
        wait "$pid" >/dev/null 2>&1 || true
    done
}
trap cleanup EXIT

TOTAL_GPUS=$(python3 - <<'PY'
import os
print(int(os.environ.get("SLURM_JOB_NUM_NODES", "1")) * int(os.environ.get("VLLM_GPUS_PER_NODE", "0")))
PY
)

python3 scripts/ray/wait_for_cluster.py \
    --address "$ip_head" \
    --expected-gpus "$TOTAL_GPUS" \
    --expected-nodes "$SLURM_JOB_NUM_NODES" \
    --timeout 600 \
    --poll-interval 10

echo "Launching vLLM API server on ${head_node_ip}:${API_PORT}"
controller_log="{experiments_dir}/logs/${job_name}_controller.log"
echo "Controller environment variables:"
echo "  TRITON_CC=$TRITON_CC"
echo "  TRITON_LIBCUDA_PATH=${TRITON_LIBCUDA_PATH:-<not set>}"
echo "  LD_LIBRARY_PATH=$LD_LIBRARY_PATH"
echo "  PATH=$PATH"
echo "  HF_HOME=$HF_HOME"
echo "  PYTHONPATH=$PYTHONPATH"

# Build env command with optional TRITON_LIBCUDA_PATH
CONTROLLER_ENV_CMD="env TRITON_CC=$TRITON_CC LD_LIBRARY_PATH=$LD_LIBRARY_PATH PATH=$PATH HF_HOME=$HF_HOME PYTHONPATH=$PYTHONPATH"
if [ -n "${TRITON_LIBCUDA_PATH:-}" ]; then
    CONTROLLER_ENV_CMD="$CONTROLLER_ENV_CMD TRITON_LIBCUDA_PATH=$TRITON_LIBCUDA_PATH"
fi

srun --export="$SRUN_EXPORT_ENV" --nodes=1 --ntasks=1 --overlap -w "$head_node" \
    $CONTROLLER_ENV_CMD \
        python3 scripts/vllm/start_vllm_ray_controller.py \
            --ray-address "$ip_head" \
            --host "$head_node_ip" \
            --port "$API_PORT" \
            --endpoint-json "$ENDPOINT_JSON" \
            --tensor-parallel-size "$TP_SIZE" \
            --pipeline-parallel-size "$PP_SIZE" \
            --data-parallel-size "$DP_SIZE" \
        >> "$controller_log" 2>&1 &
VLLM_PID=$!

python3 scripts/vllm/wait_for_endpoint.py \
    --endpoint-json "$ENDPOINT_JSON" \
    --max-attempts "${VLLM_HEALTH_MAX_ATTEMPTS:-120}" \
    --retry-delay "${VLLM_HEALTH_RETRY_DELAY:-15}" \
    --health-path "v1/models"

echo "=== vLLM Ray server is ready ==="
echo "Endpoint JSON: $ENDPOINT_JSON"
echo "Ray address: $RAY_ADDRESS"
echo "Log file: $controller_log"

wait "$VLLM_PID"
exit $?
fi

echo "=== Local vLLM Server Configuration (Legacy) ==="
echo "Model: $MODEL_PATH"
echo "Tensor Parallel Size: $TP_SIZE"
echo "Custom Model Name: $CUSTOM_MODEL_NAME"
echo "Endpoint JSON: $ENDPOINT_JSON"
HF_OVERRIDES="${VLLM_HF_OVERRIDES:-}"
MAX_NUM_SEQS="${VLLM_MAX_NUM_SEQS:-}"
GPU_MEMORY_UTILIZATION="${VLLM_GPU_MEMORY_UTILIZATION:-}"
ENABLE_EXPERT_PARALLEL="${VLLM_ENABLE_EXPERT_PARALLEL:-}"
SWAP_SPACE="${VLLM_SWAP_SPACE:-}"
MAX_SEQ_LEN_TO_CAPTURE="${VLLM_MAX_SEQ_LEN_TO_CAPTURE:-}"
MAX_MODEL_LEN="${VLLM_MAX_MODEL_LEN:-}"
TRUST_REMOTE_CODE="${VLLM_TRUST_REMOTE_CODE:-}"
DISABLE_LOG_REQUESTS="${VLLM_DISABLE_LOG_REQUESTS:-}"
CPU_OFFLOAD_GB="${VLLM_CPU_OFFLOAD_GB:-}"
KV_OFFLOADING_SIZE="${VLLM_KV_OFFLOADING_SIZE:-}"
KV_OFFLOADING_BACKEND="${VLLM_KV_OFFLOADING_BACKEND:-}"

echo "HF Overrides: ${HF_OVERRIDES:-<none>}"
if [ -n "$MAX_MODEL_LEN" ]; then
    echo "Max model len: $MAX_MODEL_LEN"
fi
if [ -n "$CPU_OFFLOAD_GB" ]; then
    echo "CPU offload: ${CPU_OFFLOAD_GB} GiB"
fi
if [ -n "$KV_OFFLOADING_SIZE" ]; then
    echo "KV offloading size: ${KV_OFFLOADING_SIZE} GiB (backend: ${KV_OFFLOADING_BACKEND:-<default>})"
fi
echo "=================================="

CMD=(python3 scripts/vllm/start_vllm_cluster.py
    --model "$MODEL_PATH"
    --num-replicas "${VLLM_NUM_REPLICAS:-1}"
    --tensor-parallel-size "$TP_SIZE"
    --pipeline-parallel-size "$PP_SIZE"
    --enable-pinggy
    --pinggy-output-path "$ENDPOINT_JSON"
    --verbose
)

if [ -n "$CUSTOM_MODEL_NAME" ]; then
    CMD+=(--custom-model-name "$CUSTOM_MODEL_NAME")
fi

if [ -n "$HF_OVERRIDES" ]; then
    CMD+=(--hf-overrides "$HF_OVERRIDES")
fi

if [ -n "${VLLM_PINGGY_HEALTH_INTERVAL:-}" ]; then
    CMD+=(--pinggy-health-interval "$VLLM_PINGGY_HEALTH_INTERVAL")
fi
if [ -n "${VLLM_PINGGY_HEALTH_TIMEOUT:-}" ]; then
    CMD+=(--pinggy-health-timeout "$VLLM_PINGGY_HEALTH_TIMEOUT")
fi
if [ -n "${VLLM_PINGGY_MAX_RESTARTS:-}" ]; then
    CMD+=(--pinggy-max-restarts "$VLLM_PINGGY_MAX_RESTARTS")
fi
if [ -n "${VLLM_PINGGY_HEALTH_URL:-}" ]; then
    CMD+=(--pinggy-health-url "$VLLM_PINGGY_HEALTH_URL")
fi

if [ -n "$MAX_NUM_SEQS" ]; then
    CMD+=(--max-num-seqs "$MAX_NUM_SEQS")
fi

if [ -n "$GPU_MEMORY_UTILIZATION" ]; then
    CMD+=(--gpu-memory-utilization "$GPU_MEMORY_UTILIZATION")
fi

if [ "$ENABLE_EXPERT_PARALLEL" = "1" ]; then
    CMD+=(--enable-expert-parallel)
fi

if [ -n "$SWAP_SPACE" ]; then
    CMD+=(--swap-space "$SWAP_SPACE")
fi

if [ -n "$CPU_OFFLOAD_GB" ]; then
    CMD+=(--cpu-offload-gb "$CPU_OFFLOAD_GB")
fi

if [ -n "$KV_OFFLOADING_SIZE" ]; then
    CMD+=(--kv-offloading-size "$KV_OFFLOADING_SIZE")
fi

if [ -n "$KV_OFFLOADING_BACKEND" ]; then
    CMD+=(--kv-offloading-backend "$KV_OFFLOADING_BACKEND")
fi

if [ -n "$MAX_SEQ_LEN_TO_CAPTURE" ]; then
    CMD+=(--max-seq-len-to-capture "$MAX_SEQ_LEN_TO_CAPTURE")
fi
if [ -n "$MAX_MODEL_LEN" ]; then
    CMD+=(--max-model-len "$MAX_MODEL_LEN")
fi

if [ "$TRUST_REMOTE_CODE" = "1" ]; then
    CMD+=(--trust-remote-code)
fi

if [ "$DISABLE_LOG_REQUESTS" = "1" ]; then
    CMD+=(--disable-log-requests)
fi

echo "Starting legacy vLLM server:"
echo "  ${CMD[*]}"
"${CMD[@]}" 2>&1 | tee {experiments_dir}/logs/vllm_server_%j.log
