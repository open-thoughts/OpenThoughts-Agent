from __future__ import annotations

import argparse
import difflib
import re
from dataclasses import dataclass
import json
from datetime import datetime
import subprocess
from pathlib import Path
from typing import Iterable, List, Tuple


@dataclass
class DetectionResult:
    pip_unpinned: list[str]
    pip_pinned: list[str]
    apt_unpinned: list[str]
    apt_pinned: list[str]


@dataclass
class PinProposal:
    # map package -> specifier to inject (e.g., {"numpy": "numpy<2"})
    pip_pins: dict[str, str]
    # map apt package -> version string to inject (if known). If unknown, leave empty.
    apt_pins: dict[str, str]


def _split_shell_words(s: str) -> list[str]:
    return re.findall(r"[^\s'\"]+|\'[^\']*\'|\"[^\"]*\"", s)


def detect_installs_from_dockerfile(dockerfile: Path) -> DetectionResult:
    content = dockerfile.read_text()
    pip_unpinned: list[str] = []
    pip_pinned: list[str] = []
    apt_unpinned: list[str] = []
    apt_pinned: list[str] = []

    lines = content.splitlines()
    i = 0
    while i < len(lines):
        raw_line = lines[i]
        line = raw_line.strip()
        if not line or line.startswith("#"):
            i += 1
            continue
        if not line.upper().startswith("RUN "):
            i += 1
            continue

        # Normalize to ease parsing of chained commands. Join continued lines ending with '\'.
        run_cmd = line[4:].rstrip()
        while run_cmd.endswith("\\") and (i + 1) < len(lines):
            run_cmd = run_cmd[:-1].rstrip() + " " + lines[i + 1].strip()
            i += 1

        parts = re.split(r"&&|;|\|\|", run_cmd)
        for part in parts:
            cmd = part.strip()
            if not cmd:
                continue

            # pip install
            if re.search(r"\bpip(3)?\s+install\b", cmd):
                tokens = _split_shell_words(cmd)
                # collect packages after 'install' and skip flags starting with '-'
                try:
                    idx = next(i for i, t in enumerate(tokens) if t == "install")
                except StopIteration:
                    continue
                pkgs = [t for t in tokens[idx + 1 :] if not t.startswith("-")]
                for p in pkgs:
                    # strip extras like package[extra]
                    name = re.split(r"[<>=!~\[]", p, maxsplit=1)[0]
                    has_spec = bool(re.search(r"[<>=!~]", p))
                    (pip_pinned if has_spec else pip_unpinned).append(name)

            # uv add / uv pip install
            if re.search(r"\buv\s+(add|pip\s+install)\b", cmd):
                tokens = _split_shell_words(cmd)
                # find the subcommand index
                try:
                    if "add" in tokens:
                        idx = tokens.index("add")
                    else:
                        idx = tokens.index("install")
                except ValueError:
                    idx = -1
                if idx != -1:
                    pkgs = [t for t in tokens[idx + 1 :] if not t.startswith("-")]
                    for p in pkgs:
                        name = re.split(r"[<>=!~\[]", p, maxsplit=1)[0]
                        has_spec = bool(re.search(r"[<>=!~]", p))
                        (pip_pinned if has_spec else pip_unpinned).append(name)

            # apt-get install / apt install
            if re.search(r"\bapt(-get)?\s+install\b", cmd):
                tokens = _split_shell_words(cmd)
                try:
                    idx = next(i for i, t in enumerate(tokens) if t == "install")
                except StopIteration:
                    continue
                pkgs = [t for t in tokens[idx + 1 :] if not t.startswith("-")]
                for p in pkgs:
                    # version pin looks like pkg=1.2.3-1
                    if "=" in p and not p.startswith("http"):
                        apt_pinned.append(p.split("=", 1)[0])
                    else:
                        apt_unpinned.append(p)
        i += 1
    # de-dup while preserving order
    def _dedup(xs: Iterable[str]) -> list[str]:
        seen: set[str] = set()
        out: list[str] = []
        for x in xs:
            if x not in seen:
                seen.add(x)
                out.append(x)
        return out

    return DetectionResult(
        pip_unpinned=_dedup(pip_unpinned),
        pip_pinned=_dedup(pip_pinned),
        apt_unpinned=_dedup(apt_unpinned),
        apt_pinned=_dedup(apt_pinned),
    )


def propose_pins(d: DetectionResult) -> PinProposal:
    pip_pins: dict[str, str] = {}
    # Heuristics for known fragile packages
    for name in d.pip_unpinned:
        if name.lower() == "numpy":
            pip_pins[name] = "numpy<2"
        elif name.lower() == "fasttext":
            pip_pins[name] = "fasttext==0.9.2"
        else:
            # leave as a note to resolve exact version later
            pip_pins[name] = f"{name}==<RESOLVE_VERSION>"

    # keep existing pins intact; do not override
    apt_pins: dict[str, str] = {name: "<RESOLVE_VERSION>" for name in d.apt_unpinned}
    return PinProposal(pip_pins=pip_pins, apt_pins=apt_pins)


def rewrite_dockerfile_with_pins(
    dockerfile: Path,
    proposal: PinProposal,
    *,
    use_requirements: bool = False,
    requirements_filename: str | None = None,
    apt_prefs_filename: str | None = None,
) -> Tuple[str, str]:
    original = dockerfile.read_text()
    modified = original

    # Rewrite pip installs by replacing bare names with our proposed specifiers
    def _rewrite_pip_line(cmd: str) -> str:
        tokens = _split_shell_words(cmd)
        try:
            idx = next(i for i, t in enumerate(tokens) if t == "install")
        except StopIteration:
            return cmd
        out: list[str] = []
        for i, t in enumerate(tokens):
            if i <= idx:
                out.append(t)
                continue
            if t.startswith("-"):
                out.append(t)
                continue
            name = re.split(r"[<>=!~\[]", t, maxsplit=1)[0]
            if name in proposal.pip_pins:
                out.append(proposal.pip_pins[name])
            else:
                out.append(t)
        return " ".join(out)

    new_lines: list[str] = []
    inserted_req_copy = False
    inserted_apt_prefs_copy = False
    pending_apt_update = False  # track a standalone RUN apt-get update to merge with next install
    lines2 = original.splitlines()
    li = 0
    while li < len(lines2):
        raw_line = lines2[li]
        line = raw_line
        if line.strip().upper().startswith("RUN "):
            # Accumulate backslash-continued RUN
            run_cmd = line[4:].rstrip()
            while run_cmd.endswith("\\") and (li + 1) < len(lines2):
                run_cmd = run_cmd[:-1].rstrip() + " " + lines2[li + 1].strip()
                li += 1
            # If this RUN contains only an apt update (no install), defer it to merge
            if (
                re.search(r"\bapt(-get)?\s+update\b", run_cmd)
                and not re.search(r"\bapt(-get)?\s+install\b", run_cmd)
            ):
                pending_apt_update = True
                # Skip emitting this line; will be merged into next install RUN
                continue

            # If next RUN contains apt install and we had a pending update, prepend it
            if pending_apt_update and re.search(r"\bapt(-get)?\s+install\b", run_cmd):
                if not re.search(r"\bapt(-get)?\s+update\b", run_cmd):
                    run_cmd = f"apt-get update && {run_cmd}"
                pending_apt_update = False
            # Only rewrite pip invocations; leave others intact
            def _rewrite_sub(part: str) -> str:
                s = part
                if re.search(r"\bpip(3)?\s+install\b", s) or re.search(
                    r"\buv\s+(add|pip\s+install)\b", s
                ):
                    if use_requirements and requirements_filename:
                        # Replace entire pip install with -r requirements
                        return re.sub(
                            r"\bpip(3)?\s+install\b.*$",
                            (
                                f"python3 -m pip install -r /tmp/{requirements_filename} "
                                f"|| pip install -r /tmp/{requirements_filename}"
                            ),
                            s,
                        )
                    else:
                        return _rewrite_pip_line(s)
                # Rewrite apt installs by injecting =VERSION when available
                if re.search(r"\bapt(-get)?\s+install\b", s):
                    tokens = _split_shell_words(s)
                    try:
                        idx = next(i for i, t in enumerate(tokens) if t == "install")
                    except StopIteration:
                        return s
                    new_tokens: list[str] = []
                    # We previously appended apt-mark hold for replaced names, but we now
                    # rely solely on pkg=version and remove holds for speed/simplicity.
                    for i, t in enumerate(tokens):
                        if i <= idx:
                            new_tokens.append(t)
                            continue
                        if t.startswith("-"):
                            new_tokens.append(t)
                            continue
                        name = t.split("=", 1)[0]
                        version = proposal.apt_pins.get(name)
                        if version and version != "<RESOLVE_VERSION>":
                            new_tokens.append(f"{name}={version}")
                        else:
                            new_tokens.append(t)
                    s = " ".join(new_tokens)
                    return s
                # Strip any apt-mark hold segments by replacing with a no-op
                if re.search(r"\bapt-mark\s+hold\b", s):
                    return ":"
                return s

            segments = re.split(r"(&&|;|\|\|)", run_cmd)
            for i in range(0, len(segments), 2):
                segments[i] = _rewrite_sub(segments[i])
            # Join with clean spacing around control operators and backslashes
            if segments:
                rebuilt = segments[0].strip()
                j = 1
                while j < len(segments):
                    delim = segments[j]
                    nxt = segments[j + 1] if j + 1 < len(segments) else ""
                    nxt = nxt.strip()
                    if delim in ("&&", "||", ";"):
                        rebuilt += f" {delim} {nxt}"
                    else:
                        rebuilt += delim + nxt
                    j += 2
            else:
                rebuilt = run_cmd.strip()
            # Insert COPY lines immediately before first relevant RUN
            if use_requirements and (re.search(r"\bpip(3)?\s+install\b", run_cmd) or re.search(r"\buv\s+(add|pip\s+install)\b", run_cmd)) and requirements_filename and not inserted_req_copy:
                new_lines.append(f"COPY {requirements_filename} /tmp/{requirements_filename}")
                inserted_req_copy = True
            if apt_prefs_filename and re.search(r"\bapt(-get)?\s+install\b", run_cmd) and not inserted_apt_prefs_copy:
                new_lines.append(
                    f"COPY {apt_prefs_filename} /etc/apt/preferences.d/zzz-task-pins"
                )
                inserted_apt_prefs_copy = True
            new_lines.append("RUN " + rebuilt)
        else:
            new_lines.append(line)
        li += 1

    modified = "\n".join(new_lines)
    # If there was a pending apt update we never merged, append it back unchanged
    if pending_apt_update:
        modified = modified + "\nRUN apt-get update"
    return original, modified


def _is_hw_sensitive_pip(name: str, version: str) -> bool:
    n = name.lower()
    v = (version or "").lower()
    patterns = [
        "cuda",
        "cudnn",
        "cublas",
        "rocm",
        "tensorrt",
        "nvidia",
        "cupy",
        "opencl",
        "mkl",
        "xpu",
        "oneapi",
        "metal",
        "mps",
    ]
    if any(p in n for p in patterns):
        return True
    if "+cu" in v or "+rocm" in v or "+cpu" in v or "+mps" in v or "+metal" in v:
        return True
    return False


def _is_hw_sensitive_apt(name: str, version: str) -> bool:
    n = name.lower()
    patterns = [
        "nvidia-",
        "cuda-",
        "libcuda",
        "libnvidia-",
        "linux-image-",
        "linux-headers-",
        "microcode",
        "firmware-",
        "rocm-",
        "amdgpu-",
        "tensorrt",
        "intel-",
    ]
    return any(p in n for p in patterns)


def _write_requirements_lock(
    env_dir: Path, pip_closure: dict[str, str], extra_unpinned: set[str] | None = None
) -> Path:
    req_path = env_dir / "requirements.lock"
    lines: list[str] = []
    extra_unpinned = extra_unpinned or set()
    for name in sorted(pip_closure):
        ver = pip_closure[name]
        if _is_hw_sensitive_pip(name, ver) or name in extra_unpinned:
            lines.append(f"{name}")  # leave unpinned
        else:
            lines.append(f"{name}=={ver}")
    req_path.write_text("\n".join(lines) + "\n")
    return req_path


def _write_apt_prefs(
    env_dir: Path, apt_closure: dict[str, str], extra_skip: set[str] | None = None
) -> Path:
    prefs_path = env_dir / "apt-pins.pref"
    blocks: list[str] = []
    extra_skip = extra_skip or set()
    for name in sorted(apt_closure):
        ver = apt_closure[name]
        if _is_hw_sensitive_apt(name, ver) or name in extra_skip:
            continue  # do not pin
        blocks.append(
            "\n".join(
                [
                    f"Package: {name}",
                    f"Pin: version {ver}",
                    "Pin-Priority: 1001",
                    "",
                ]
            )
        )
    prefs_path.write_text("".join(blocks))
    return prefs_path


def _write_env_sensitivity(env_dir: Path, pip_diff: dict[str, tuple[str, str]], apt_diff: dict[str, tuple[str, str]]) -> Path:
    path = env_dir / "env-sensitivity.json"
    data = {
        "created_at": datetime.now().isoformat(timespec="seconds"),
        "pip": {k: {"docker": v[0], "daytona": v[1]} for k, v in pip_diff.items()},
        "apt": {k: {"docker": v[0], "daytona": v[1]} for k, v in apt_diff.items()},
    }
    path.write_text(json.dumps(data, indent=2) + "\n")
    return path


def _read_env_sensitivity(env_dir: Path) -> tuple[set[str], set[str], dict | None]:
    path = env_dir / "env-sensitivity.json"
    if not path.exists():
        return set(), set(), None
    try:
        data = json.loads(path.read_text())
        pip_names = set((data.get("pip") or {}).keys())
        apt_names = set((data.get("apt") or {}).keys())
        return pip_names, apt_names, data
    except Exception:
        return set(), set(), None


def _append_env_sensitivity_note(
    task_dir: Path, pip_diff: dict[str, tuple[str, str]], apt_diff: dict[str, tuple[str, str]]
):
    md_path = task_dir / "env-notes.md"
    note_lines: list[str] = []
    note_lines.append("")
    note_lines.append("---")
    note_lines.append(
        f"Env Sensitivity Note ({datetime.now().isoformat(timespec='seconds')}):"
    )
    if pip_diff:
        note_lines.append("pip packages differing between docker and daytona:")
        for name in sorted(pip_diff):
            dv, yv = pip_diff[name]
            note_lines.append(f"- {name}: docker={dv} daytona={yv}")
    if apt_diff:
        note_lines.append("apt packages differing between docker and daytona:")
        for name in sorted(apt_diff):
            dv, yv = apt_diff[name]
            note_lines.append(f"- {name}: docker={dv} daytona={yv}")
    prefix = md_path.read_text() if md_path.exists() else ""
    md_path.write_text(prefix + ("\n" if prefix and not prefix.endswith("\n") else "") + "\n".join(note_lines) + "\n")


def _parse_final_base_image(dockerfile: Path) -> str | None:
    """Return the image used by the final stage (last FROM) or None if not found."""
    img: str | None = None
    for raw in dockerfile.read_text().splitlines():
        line = raw.strip()
        if not line or line.startswith("#"):
            continue
        if line.upper().startswith("FROM "):
            # Syntax: FROM <image> [AS <name>]
            parts = line.split()
            if len(parts) >= 2:
                img = parts[1]
    return img


def _docker_run(image: str, cmd: str) -> tuple[int, str]:
    p = subprocess.run(
        ["docker", "run", "--rm", image, "bash", "-lc", cmd],
        capture_output=True,
        text=True,
    )
    out = p.stdout.strip()
    if p.stderr:
        out = (out + "\n" + p.stderr.strip()).strip()
    return p.returncode, out


def _snapshot_pip(image: str) -> dict[str, str]:
    """Return mapping of installed Python packages to exact versions in given image."""
    rc, out = _docker_run(
        image,
        # Try common options; tolerate absence
        (
            "python3 -m pip freeze 2>/dev/null || "
            "pip freeze 2>/dev/null || "
            "true"
        ),
    )
    pkgs: dict[str, str] = {}
    for line in out.splitlines():
        line = line.strip()
        # Lines usually like name==version; skip VCS/editable/markers for now
        if not line or line.startswith("-") or line.startswith("#"):
            continue
        if "==" in line:
            name, ver = line.split("==", 1)
            pkgs[name.strip()] = ver.strip()
    return pkgs


def _snapshot_apt(image: str) -> dict[str, str]:
    """Return mapping of installed apt packages to exact versions in given image."""
    rc, out = _docker_run(
        image,
        "dpkg-query -W -f='${Package}=${Version}\n' 2>/dev/null || true",
    )
    pkgs: dict[str, str] = {}
    for line in out.splitlines():
        line = line.strip()
        if not line or "=" not in line:
            continue
        name, ver = line.split("=", 1)
        name = name.strip()
        ver = ver.strip()
        if name:
            pkgs[name] = ver
    return pkgs


def _dict_diff_added_or_changed(base: dict[str, str], final: dict[str, str]) -> dict[str, str]:
    """Return items that are new or changed in final vs base."""
    out: dict[str, str] = {}
    for k, v in final.items():
        if k not in base or base[k] != v:
            out[k] = v
    return out


def build_image_and_resolve_versions(
    task_dir: Path, d: DetectionResult, tag: str
) -> tuple[dict[str, str], dict[str, str]]:
    """Build the task image and resolve exact versions for all effective installs.

    Returns (pip_versions, apt_versions) for the FULL closure of packages added or
    changed by this Dockerfile relative to its base image, not just direct refs.
    """
    env_dir = task_dir / "environment"
    if not env_dir.exists():
        raise FileNotFoundError(f"Environment directory not found: {env_dir}")

    # Build image
    print(f"[resolve] docker build -t {tag} {env_dir}")
    subprocess.run([
        "docker", "build", "-t", tag, str(env_dir)
    ], check=True)

    # Determine base image (final stage)
    dockerfile = env_dir / "Dockerfile"
    base_img = _parse_final_base_image(dockerfile)
    if not base_img:
        print("[resolve] WARN: could not parse base image; reporting final state only.")
        base_pip: dict[str, str] = {}
        base_apt: dict[str, str] = {}
    else:
        print(f"[resolve] Using base image: {base_img}")
        try:
            subprocess.run(["docker", "pull", base_img], check=False)
        except Exception:
            pass
        base_pip = _snapshot_pip(base_img)
        base_apt = _snapshot_apt(base_img)

    # Snapshot final image
    final_pip = _snapshot_pip(tag)
    final_apt = _snapshot_apt(tag)

    # Compute closure: new or changed vs base
    pip_versions = _dict_diff_added_or_changed(base_pip, final_pip)
    apt_versions = _dict_diff_added_or_changed(base_apt, final_apt)

    return pip_versions, apt_versions


def unified_diff(a: str, b: str, path: Path) -> str:
    diff = difflib.unified_diff(
        a.splitlines(keepends=True),
        b.splitlines(keepends=True),
        fromfile=str(path),
        tofile=str(path),
    )
    return "".join(diff)


def main() -> None:
    parser = argparse.ArgumentParser(
        description="Pin task environment dependencies to stabilize Oracle runs"
    )
    parser.add_argument(
        "--task",
        required=True,
        type=Path,
        help="Path to task directory (containing environment/)",
    )
    parser.add_argument(
        "--apply",
        action="store_true",
        help="Apply proposed edits to the Dockerfile (otherwise dry-run)",
    )
    parser.add_argument(
        "--print-diff",
        action="store_true",
        help="Print a unified diff of Dockerfile changes",
    )
    parser.add_argument(
        "--validate",
        action="store_true",
        help=(
            "Run Oracle before and/or after changes using selected env to verify reward"
        ),
    )
    parser.add_argument(
        "--env",
        choices=["docker", "daytona"],
        default="docker",
        help="Environment to use for resolution/validation (docker|daytona). Default: docker",
    )
    parser.add_argument(
        "--trials-dir",
        type=Path,
        default=Path(".pin_trials"),
        help="Directory to store validation trial outputs (default: ./.pin_trials)",
    )
    parser.add_argument(
        "--keep-trials",
        action="store_true",
        help="Keep validation trials on disk (default: delete after run)",
    )
    parser.add_argument(
        "--post-apply-timeout-multiplier",
        type=float,
        default=2.5,
        help="Multiplier for baseline elapsed to set post-apply timeout (default: 2.5x)",
    )
    parser.add_argument(
        "--post-apply-timeout-min",
        type=float,
        default=60.0,
        help="Minimum seconds for post-apply timeout (default: 60s)",
    )
    parser.add_argument(
        "--fuzz-env",
        action="store_true",
        help=(
            "Run resolution in both docker and daytona, report env-sensitive packages, "
            "save machine-readable cache at environment/env-sensitivity.json and append "
            "a human-readable note to env-notes.md"
        ),
    )
    args = parser.parse_args()

    # Enforce: --apply implies --validate, defaulting to current --env
    if args.apply and not args.validate:
        print(f"[apply] --apply requested; enabling --validate with env={args.env}.")
        args.validate = True

    task_dir: Path = args.task.expanduser().resolve()
    dockerfile = task_dir / "environment" / "Dockerfile"
    if not dockerfile.exists():
        raise SystemExit(f"Dockerfile not found at {dockerfile}")

    detection = detect_installs_from_dockerfile(dockerfile)
    proposal = propose_pins(detection)

    print(f"Environment for resolution/validation: {args.env}")
    print("Detected installs:")
    print(f"  pip (unpinned): {detection.pip_unpinned}")
    print(f"  pip (pinned):   {detection.pip_pinned}")
    print(f"  apt (unpinned): {detection.apt_unpinned}")
    print(f"  apt (pinned):   {detection.apt_pinned}")

    print("\nProposed pip pins:")
    for k, v in proposal.pip_pins.items():
        print(f"  {k} -> {v}")
    if proposal.apt_pins:
        print("\nNote: apt packages detected without versions. Consider pinning exact versions or using apt preferences:")
        for k in proposal.apt_pins:
            print(f"  {k} -> {k}=<RESOLVE_VERSION>")

    # Resolve the FULL closure of installs relative to base (for reporting),
    # and optionally use the results to pin direct placeholders when applying.
    pip_closure: dict[str, str] = {}
    apt_closure: dict[str, str] = {}
    try:
        tag = f"sb-pin-temp:{task_dir.name}"
        # If using daytona for resolution, snapshot via environments; otherwise use docker.
        if args.env == "daytona":
            # Lazy imports to avoid mandatory dependency when not needed
            from harbor.models.trial.paths import TrialPaths
            from harbor.models.environment_type import EnvironmentType as _EnvType
            from harbor.environments.factory import EnvironmentFactory

            trial_paths = TrialPaths(trial_dir=(args.trials_dir / "_pin_env_snapshot"))
            trial_paths.mkdir()

            # Create and start environment
            env = EnvironmentFactory.create_environment(
                type=_EnvType.DAYTONA,
                environment_dir=task_dir / "environment",
                environment_name=tag,
                session_id=f"pin-{task_dir.name}",
                trial_paths=trial_paths,
            )

            import asyncio as _asyncio

            async def _snapshot_daytona() -> tuple[dict[str, str], dict[str, str]]:
                await env.start(force_build=True)
                try:
                    # pip snapshot
                    pip_cmd = (
                        "python3 -m pip freeze 2>/dev/null || pip freeze 2>/dev/null || true"
                    )
                    res = await env.exec(pip_cmd)
                    pip_pkgs: dict[str, str] = {}
                    for line in (res.stdout or "").splitlines():
                        line = line.strip()
                        if not line or line.startswith("-") or line.startswith("#"):
                            continue
                        if "==" in line:
                            n, v = line.split("==", 1)
                            pip_pkgs[n.strip()] = v.strip()

                    # apt snapshot
                    apt_cmd = "dpkg-query -W -f='${Package}=${Version}\\n' 2>/dev/null || true"
                    res2 = await env.exec(apt_cmd)
                    apt_pkgs: dict[str, str] = {}
                    for line in (res2.stdout or "").splitlines():
                        if "=" in line:
                            n, v = line.split("=", 1)
                            n = n.strip()
                            if n:
                                apt_pkgs[n] = v.strip()

                    return pip_pkgs, apt_pkgs
                finally:
                    await env.stop(delete=True)

            pip_final, apt_final = _asyncio.run(_snapshot_daytona())

            # Compute base via docker (best-effort) and then diff
            base_img = _parse_final_base_image(dockerfile)
            if base_img:
                try:
                    subprocess.run(["docker", "pull", base_img], check=False)
                except Exception:
                    pass
                base_pip = _snapshot_pip(base_img)
                base_apt = _snapshot_apt(base_img)
            else:
                base_pip = {}
                base_apt = {}

            pip_closure = _dict_diff_added_or_changed(base_pip, pip_final)
            apt_closure = _dict_diff_added_or_changed(base_apt, apt_final)
        else:
            pip_closure, apt_closure = build_image_and_resolve_versions(
                task_dir, detection, tag
            )

        if args.fuzz_env:
            # Compute both ways to see differences regardless of selected env
            docker_pip, docker_apt = build_image_and_resolve_versions(
                task_dir, detection, tag + "-docker"
            )

            # Daytona snapshot (reuse code above but force daytona)
            from harbor.models.trial.paths import TrialPaths as _TP
            from harbor.models.environment_type import EnvironmentType as _EnvType
            from harbor.environments.factory import EnvironmentFactory as _EF
            import asyncio as _asyncio

            trial_paths2 = _TP(trial_dir=(args.trials_dir / "_pin_env_snapshot_daytona"))
            trial_paths2.mkdir()
            env2 = _EF.create_environment(
                type=_EnvType.DAYTONA,
                environment_dir=task_dir / "environment",
                environment_name=tag + "-daytona",
                session_id=f"pin-{task_dir.name}-daytona",
                trial_paths=trial_paths2,
            )

            async def _snap2():
                await env2.start(force_build=True)
                try:
                    r1 = await env2.exec(
                        "python3 -m pip freeze 2>/dev/null || pip freeze 2>/dev/null || true"
                    )
                    pip_pkgs: dict[str, str] = {}
                    for ln in (r1.stdout or "").splitlines():
                        ln = ln.strip()
                        if not ln or ln.startswith("-") or ln.startswith("#"):
                            continue
                        if "==" in ln:
                            n, v = ln.split("==", 1)
                            pip_pkgs[n.strip()] = v.strip()
                    r2 = await env2.exec(
                        "dpkg-query -W -f='${Package}=${Version}\\n' 2>/dev/null || true"
                    )
                    apt_pkgs: dict[str, str] = {}
                    for ln in (r2.stdout or "").splitlines():
                        if "=" in ln:
                            n, v = ln.split("=", 1)
                            n = n.strip()
                            if n:
                                apt_pkgs[n] = v.strip()
                    return pip_pkgs, apt_pkgs
                finally:
                    await env2.stop(delete=True)

            daytona_pip_final, daytona_apt_final = _asyncio.run(_snap2())
            base_img2 = _parse_final_base_image(dockerfile)
            if base_img2:
                try:
                    subprocess.run(["docker", "pull", base_img2], check=False)
                except Exception:
                    pass
                base_pip2 = _snapshot_pip(base_img2)
                base_apt2 = _snapshot_apt(base_img2)
            else:
                base_pip2 = {}
                base_apt2 = {}
            daytona_pip = _dict_diff_added_or_changed(base_pip2, daytona_pip_final)
            daytona_apt = _dict_diff_added_or_changed(base_apt2, daytona_apt_final)

            # Compute diffs package->(docker_ver, daytona_ver) when versions differ
            pip_diff: dict[str, tuple[str, str]] = {}
            all_p = set(docker_pip) | set(daytona_pip)
            for n in all_p:
                dv = docker_pip.get(n)
                yv = daytona_pip.get(n)
                if dv and yv and dv != yv:
                    pip_diff[n] = (dv, yv)
            apt_diff: dict[str, tuple[str, str]] = {}
            all_a = set(docker_apt) | set(daytona_apt)
            for n in all_a:
                dv = docker_apt.get(n)
                yv = daytona_apt.get(n)
                if dv and yv and dv != yv:
                    apt_diff[n] = (dv, yv)

            if pip_diff or apt_diff:
                print("\nEnv-sensitive package differences detected:")
                if pip_diff:
                    print("  pip:")
                    for n, (dv, yv) in sorted(pip_diff.items()):
                        print(f"    {n}: docker={dv} daytona={yv}")
                if apt_diff:
                    print("  apt:")
                    for n, (dv, yv) in sorted(apt_diff.items()):
                        print(f"    {n}: docker={dv} daytona={yv}")
                _append_env_sensitivity_note(task_dir, pip_diff, apt_diff)
                # Also write a machine-readable cache inside environment/
                env_dir = task_dir / "environment"
                cache_path = _write_env_sensitivity(env_dir, pip_diff, apt_diff)
                print(f"Saved env-sensitivity cache: {cache_path}")
            else:
                print("No env-sensitive differences found between docker and daytona.")
        # Load any env-sensitivity cache to refine pin decisions
        env_dir_for_cache = task_dir / "environment"
        cached_pip_sensitive, cached_apt_sensitive, _cache_data = _read_env_sensitivity(
            env_dir_for_cache
        )

        if args.apply:
            # Fill in resolved versions for direct placeholders that we proposed,
            # guarding against hardware-specific packages.
            for name, ver in pip_closure.items():
                if proposal.pip_pins.get(name, "").endswith("<RESOLVE_VERSION>"):
                    if _is_hw_sensitive_pip(name, ver) or name in cached_pip_sensitive:
                        proposal.pip_pins[name] = name  # leave unpinned
                    else:
                        proposal.pip_pins[name] = f"{name}=={ver}"
            # For apt, only pin direct packages present in Dockerfile RUN install lines
            direct_apt_names = set(detection.apt_unpinned + detection.apt_pinned)
            for name in direct_apt_names:
                ver = apt_closure.get(name)
                if ver and (name in proposal.apt_pins):
                    if not _is_hw_sensitive_apt(name, ver) and name not in cached_apt_sensitive:
                        proposal.apt_pins[name] = ver
    except Exception as e:
        print(f"[resolve] Skipping version resolution due to error: {e}")

    # Directly inline pins into Dockerfile (no lockfiles/prefs)
    requirements_filename: str | None = None
    apt_prefs_filename: str | None = None

    original, modified = rewrite_dockerfile_with_pins(
        dockerfile,
        proposal,
        use_requirements=False,
        requirements_filename=requirements_filename,
        apt_prefs_filename=apt_prefs_filename,
    )
    if args.print_diff or not args.apply:
        print("\nDockerfile proposed changes:")
        print(unified_diff(original, modified, dockerfile))

    # Do not apply immediately; we will validate baseline first, then apply and re-run
    applied_changes = False
    pending_apply = bool(args.apply and (original != modified))
    backup: Path | None = None
    created_artifacts: list[Path] = []

    # Report full effective installs relative to base image
    if pip_closure or apt_closure:
        print("\nEffective installs relative to base (full closure):")
        if pip_closure:
            print("  pip:")
            for k, v in sorted(pip_closure.items()):
                print(f"    {k}=={v}")
        else:
            print("  pip: (no changes vs base)")
        if apt_closure:
            print("  apt:")
            for k, v in sorted(apt_closure.items()):
                print(f"    {k}={v}")
        else:
            print("  apt: (no changes vs base)")

    if args.validate:
        # Lazy import to avoid hard dependency if user only wants detection
        import asyncio
        import time
        import shutil
        from datetime import datetime

        from harbor.models.agent.name import AgentName
        from harbor.models.environment_type import EnvironmentType
        from harbor.models.trial.config import (
            AgentConfig,
            EnvironmentConfig,
            TaskConfig,
            TrialConfig,
        )
        from harbor.trial.trial import Trial

        def _run_once(label: str, timeout_sec: float | None = None) -> tuple[float | None, Path | None, float | None]:
            trials_dir: Path = args.trials_dir / f"{label}__{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            trials_dir.mkdir(parents=True, exist_ok=True)

            # Select environment type per --env
            from harbor.models.environment_type import EnvironmentType as _EnvType
            env_type = _EnvType.DOCKER if args.env == "docker" else _EnvType.DAYTONA

            config = TrialConfig(
                task=TaskConfig(path=task_dir),
                trials_dir=trials_dir,
                agent=AgentConfig(name=AgentName.ORACLE.value),
                environment=EnvironmentConfig(type=env_type, force_build=True, delete=False),
            )

            print(f"\n[validate] Running Oracle ({args.env}) (trials_dir={trials_dir})...")
            try:
                start = time.monotonic()
                if timeout_sec and timeout_sec > 0:
                    result = asyncio.run(asyncio.wait_for(Trial(config).run(), timeout=timeout_sec))
                else:
                    result = asyncio.run(Trial(config).run())
                elapsed = time.monotonic() - start
            except Exception as e:  # pragma: no cover
                print(f"[validate] Trial run failed: {e}")
                return None, trials_dir, None

            reward = (
                result.verifier_result.reward if result.verifier_result else None
            )
            print(f"[validate] Reward: {reward}")
            return reward, trials_dir, elapsed

        def _revert_changes():
            nonlocal backup
            # Restore original Dockerfile
            if backup and backup.exists():
                dockerfile.write_text(original)
                print("[validate] Reverted Dockerfile to original content due to validation failure.")
            # Remove created artifacts
            for p in created_artifacts:
                try:
                    if p.exists():
                        p.unlink()
                except Exception:
                    pass

        base_reward: float | None = None
        base_dir: Path | None = None
        base_elapsed: float | None = None
        post_reward: float | None = None
        post_dir: Path | None = None

        # Always run baseline first with current Dockerfile
        base_reward, base_dir, base_elapsed = _run_once("baseline")

        if pending_apply:
            # Apply changes now
            backup = dockerfile.with_suffix(dockerfile.suffix + ".bak")
            backup.write_text(original)
            dockerfile.write_text(modified)
            print(f"Updated {dockerfile} (backup at {backup})")
            applied_changes = True

            # Prewarm Docker build caches to avoid counting build time against timeout
            try:
                env_dir = task_dir / "environment"
                print(f"[validate] Prewarming Docker build cache: docker build {env_dir}")
                subprocess.run([
                    "docker", "build", "-t", f"sb-prewarm:{task_dir.name}", str(env_dir)
                ], check=False)
            except Exception as e:
                print(f"[validate] Prewarm build failed (non-fatal): {e}")

            # Compute timeout as multiplier x baseline, with a floor
            timeout_sec = None
            if base_elapsed and base_elapsed > 0:
                timeout_sec = max(base_elapsed * float(args.post_apply_timeout_multiplier), float(args.post_apply_timeout_min))
                print(
                    f"[validate] Post-pin run timeout set to {timeout_sec:.1f}s "
                    f"({args.post_apply_timeout_multiplier}x baseline {base_elapsed:.1f}s, min {args.post_apply_timeout_min:.0f}s)"
                )

            # Post-pin run with timeout
            post_reward, post_dir, _ = _run_once("post_pin", timeout_sec=timeout_sec)
            if post_reward is None:
                print("[validate] Environment failed to launch/crashed or timed out during post-pin validation. Reverting pins.")
                # Revert
                if backup and backup.exists():
                    dockerfile.write_text(original)
                    print("[validate] Reverted Dockerfile to original content due to validation failure.")
                return
            if isinstance(post_reward, (int, float)) and float(post_reward) == 0.0:
                print("[validate] Reward is 0.0 after applying pins. Reverting pins.")
                if backup and backup.exists():
                    dockerfile.write_text(original)
                    print("[validate] Reverted Dockerfile to original content due to zero reward.")
                return

        if not args.keep_trials:
            for d in [base_dir, post_dir]:
                if d and d.exists():
                    shutil.rmtree(d, ignore_errors=True)
        else:
            print(
                f"[validate] Trials kept under {args.trials_dir}. Inspect result.json for details."
            )

        # Simple summary
        print("\nSummary:")
        if base_reward is not None:
            print(f"  Baseline reward: {base_reward}")
        if post_reward is not None:
            print(f"  Post-pin reward: {post_reward}")


if __name__ == "__main__":
    main()
